{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZTYk7A+/tfT6Xn+nbImVy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niraj-K-fin/Regime-Switching-Reinforcement-Learning-Portfolio-Manager/blob/main/project_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\" Regime-Switching Reinforcement Learning Portfolio Manager \"**\n",
        "\n",
        "~ Niraj Kumar (*nirajjuly12@gmail.com*)\n",
        "\n",
        "[LinkedIn](https://www.linkedin.com/in/nirajkofficial/) | [Instagram](https://www.instagram.com/nirajkumar_real/) | [GitHub](https://github.com/Niraj-K-fin)\n"
      ],
      "metadata": {
        "id": "1IMC-WTQkIxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Installing Dependencies...*"
      ],
      "metadata": {
        "id": "TG02gOeVjut4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2LsZnnZhek0"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium==0.29.1\n",
        "!pip install stable-baselines3==2.1.0\n",
        "!pip install yfinance\n",
        "!pip install ta\n",
        "!pip install tensorboard\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Importing Modules...*"
      ],
      "metadata": {
        "id": "_sr_T6nwkEBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from ta.volatility import BollingerBands, AverageTrueRange\n",
        "from ta.momentum import RSIIndicator\n",
        "from ta.trend import MACD, EMAIndicator\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "id": "MqJ4VETmiWhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enhanced Custom Portfolio Environment** (*Highly Robust* )"
      ],
      "metadata": {
        "id": "fvp1moM-lIS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedPortfolioEnv(gym.Env):\n",
        "\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, tickers, initial_balance=100000, window_size=30, transaction_cost=0.001,\n",
        "                 stop_loss_pct=0.07, take_profit_pct=0.15, risk_target=0.01):\n",
        "        super().__init__()\n",
        "        self.tickers = tickers\n",
        "        self.n_assets = len(tickers)\n",
        "        self.initial_balance = initial_balance\n",
        "        self.window_size = window_size\n",
        "        self.transaction_cost = transaction_cost\n",
        "        self.stop_loss_pct = stop_loss_pct\n",
        "        self.take_profit_pct = take_profit_pct\n",
        "        self.risk_target = risk_target\n",
        "\n",
        "        self._download_data()\n",
        "\n",
        "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.n_assets,), dtype=np.float32)\n",
        "\n",
        "        obs_len = self.n_assets * (self.window_size + 6) + 3 + len(self.macro_indicators.columns)\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_len,), dtype=np.float32)\n",
        "        self.reset()\n",
        "\n",
        "    def _download_data(self):\n",
        "        macro_tickers = ['^VIX', '^TNX']\n",
        "        all_tickers = list(set(self.tickers + macro_tickers))\n",
        "        try:\n",
        "            df = yf.download(all_tickers, period=\"4y\")\n",
        "            self.price_data = df['Adj Close'] if 'Adj Close' in df else df['Close']\n",
        "            self.price_data = self.price_data.dropna()\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading data: {e}\")\n",
        "            raise e\n",
        "        self.dates = self.price_data.index\n",
        "\n",
        "        self.returns = self.price_data[self.tickers].pct_change().dropna()\n",
        "\n",
        "        self.regimes = self._calculate_regimes()\n",
        "\n",
        "        self.macro_indicators = self._calculate_macro_indicators()\n",
        "\n",
        "    def _calculate_regimes(self):\n",
        "        regimes = pd.DataFrame(index=self.price_data.index, columns=self.tickers)\n",
        "        for ticker in self.tickers:\n",
        "            price = self.price_data[ticker]\n",
        "            bb = BollingerBands(close=price, window=20, window_dev=2)\n",
        "            rsi = RSIIndicator(close=price, window=14)\n",
        "            macd = MACD(close=price)\n",
        "            bull = (price > bb.bollinger_hband()) & (rsi.rsi() > 60) & (macd.macd_diff() > 0)\n",
        "            bear = (price < bb.bollinger_lband()) & (rsi.rsi() < 40) & (macd.macd_diff() < 0)\n",
        "            regimes[ticker] = 1\n",
        "            regimes.loc[bull, ticker] = 2\n",
        "            regimes.loc[bear, ticker] = 0\n",
        "        return regimes.fillna(1).astype(int)\n",
        "\n",
        "    def _calculate_macro_indicators(self):\n",
        "        macro_df = pd.DataFrame(index=self.price_data.index)\n",
        "        macro_df['VIX'] = self.price_data['^VIX'] if '^VIX' in self.price_data else 0\n",
        "        macro_df['10Y'] = self.price_data['^TNX'] if '^TNX' in self.price_data else 0\n",
        "        spy = self.price_data['SPY'] if 'SPY' in self.price_data else self.price_data.iloc[:, 0]\n",
        "        macro_df['SPY_RSI'] = RSIIndicator(close=spy, window=14).rsi()\n",
        "        macro_df['SPY_MACD'] = MACD(close=spy).macd_diff()\n",
        "        macro_df.fillna(method='bfill', inplace=True)\n",
        "        return macro_df\n",
        "\n",
        "    def _get_obs(self):\n",
        "        obs = []\n",
        "        for ticker in self.tickers:\n",
        "            ret_window = self.returns[ticker].iloc[self.current_step - self.window_size:self.current_step].values\n",
        "            if len(ret_window) < self.window_size:\n",
        "                ret_window = np.pad(ret_window, (self.window_size - len(ret_window), 0), 'constant')\n",
        "            obs.extend(ret_window)\n",
        "\n",
        "            regime = self.regimes[ticker].iloc[self.current_step]\n",
        "            regime_one_hot = [0, 0, 0]\n",
        "            regime_one_hot[regime] = 1\n",
        "            obs.extend(regime_one_hot)\n",
        "\n",
        "            price = self.price_data[ticker].iloc[self.current_step - self.window_size:self.current_step]\n",
        "            if len(price) >= 50:\n",
        "                ema50 = EMAIndicator(close=price, window=50).ema_indicator().iloc[-1]\n",
        "            else:\n",
        "                ema50 = price.iloc[-1]\n",
        "            if len(price) >= 14:\n",
        "                atr = AverageTrueRange(high=price, low=price, close=price, window=14).average_true_range().iloc[-1]\n",
        "                rsi = RSIIndicator(close=price, window=14).rsi().iloc[-1]\n",
        "            else:\n",
        "                atr = 0\n",
        "                rsi = 50\n",
        "            obs.extend([atr, ema50, rsi])\n",
        "\n",
        "        obs.append(self.cash / self.initial_balance)\n",
        "        obs.append(self.total_portfolio_value / self.initial_balance)\n",
        "        obs.append(self.portfolio_volatility)\n",
        "\n",
        "        obs.extend(self.macro_indicators.iloc[self.current_step].values)\n",
        "        return np.array(obs, dtype=np.float32)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        self.current_step = self.window_size + 50\n",
        "        self.cash = self.initial_balance\n",
        "        self.shares_held = np.zeros(self.n_assets, dtype=np.float32)\n",
        "        self.avg_price = np.zeros(self.n_assets, dtype=np.float32)\n",
        "        self.total_portfolio_value = self.initial_balance\n",
        "        self.portfolio_volatility = 0.01\n",
        "        self.prev_value = self.initial_balance\n",
        "        self.stop_prices = np.zeros(self.n_assets, dtype=np.float32)\n",
        "        self.take_profit_prices = np.zeros(self.n_assets, dtype=np.float32)\n",
        "        self.trade_history = []\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        done = False\n",
        "        info = {}\n",
        "        current_prices = self.price_data[self.tickers].iloc[self.current_step].values\n",
        "\n",
        "        sum_abs = np.sum(np.abs(action))\n",
        "        if sum_abs < 1e-8:\n",
        "            target_weights = np.zeros_like(action)\n",
        "        else:\n",
        "            target_weights = action / sum_abs\n",
        "\n",
        "        recent_vol = np.std(self.returns.iloc[self.current_step-20:self.current_step].values, axis=0)\n",
        "        inv_vol = 1 / (recent_vol + 1e-4)\n",
        "        scaled_weights = target_weights * inv_vol\n",
        "        scaled_weights /= (np.sum(np.abs(scaled_weights)) + 1e-6)\n",
        "        target_alloc = scaled_weights * self.total_portfolio_value\n",
        "\n",
        "        for i in range(self.n_assets):\n",
        "            price = current_prices[i]\n",
        "            desired_shares = target_alloc[i] / price\n",
        "            delta_shares = desired_shares - self.shares_held[i]\n",
        "            cost = abs(delta_shares) * price * self.transaction_cost\n",
        "            trade_value = delta_shares * price\n",
        "            if self.cash - cost - max(0, trade_value) < 0:\n",
        "                continue\n",
        "            self.cash -= cost + max(0, trade_value)\n",
        "            self.shares_held[i] += delta_shares\n",
        "            if delta_shares > 0:\n",
        "                self.avg_price[i] = (self.avg_price[i] * (self.shares_held[i] - delta_shares) + price * delta_shares) / (self.shares_held[i] + 1e-6)\n",
        "            self.stop_prices[i] = self.avg_price[i] * (1 - self.stop_loss_pct)\n",
        "            self.take_profit_prices[i] = self.avg_price[i] * (1 + self.take_profit_pct)\n",
        "\n",
        "        for i in range(self.n_assets):\n",
        "            price = current_prices[i]\n",
        "            if self.shares_held[i] > 0:\n",
        "                if price < self.stop_prices[i] or price > self.take_profit_prices[i]:\n",
        "                    self.cash += self.shares_held[i] * price * (1 - self.transaction_cost)\n",
        "                    self.shares_held[i] = 0\n",
        "                    self.avg_price[i] = 0\n",
        "            elif self.shares_held[i] < 0:\n",
        "                if price > self.stop_prices[i] or price < self.take_profit_prices[i]:\n",
        "                    self.cash -= abs(self.shares_held[i]) * price * (1 + self.transaction_cost)\n",
        "                    self.shares_held[i] = 0\n",
        "                    self.avg_price[i] = 0\n",
        "\n",
        "        asset_values = self.shares_held * current_prices\n",
        "        self.total_portfolio_value = self.cash + asset_values.sum()\n",
        "        self.portfolio_volatility = np.std(self.returns.iloc[self.current_step-20:self.current_step].values) if self.current_step > 20 else 0.01\n",
        "\n",
        "        ret = (self.total_portfolio_value - self.prev_value) / self.prev_value\n",
        "        vol = max(self.portfolio_volatility, 1e-3)\n",
        "        reward = ret / vol\n",
        "        self.prev_value = self.total_portfolio_value\n",
        "\n",
        "        self.current_step += 1\n",
        "        if self.current_step >= len(self.price_data) - 1 or self.total_portfolio_value < self.initial_balance * 0.5:\n",
        "            done = True\n",
        "\n",
        "        obs = self._get_obs()\n",
        "        self.trade_history.append({\n",
        "            'step': self.current_step,\n",
        "            'portfolio_value': self.total_portfolio_value,\n",
        "            'cash': self.cash,\n",
        "            'shares_held': self.shares_held.copy(),\n",
        "            'prices': current_prices.copy(),\n",
        "            'action': action,\n",
        "            'reward': reward\n",
        "        })\n",
        "        return obs, reward, done, False, info\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print(f\"Step: {self.current_step}\")\n",
        "        print(f\"Portfolio Value: {self.total_portfolio_value:.2f}\")\n",
        "        print(f\"Cash: {self.cash:.2f}\")\n",
        "        print(f\"Shares Held: {self.shares_held}\")\n",
        "        print(f\"Avg Prices: {self.avg_price}\")"
      ],
      "metadata": {
        "id": "gSFy4COHirTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Environment Helper**"
      ],
      "metadata": {
        "id": "iYyuunaomPOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_env(tickers):\n",
        "    def _init():\n",
        "        env = EnhancedPortfolioEnv(tickers)\n",
        "        env = Monitor(env)\n",
        "        return env\n",
        "    return _init"
      ],
      "metadata": {
        "id": "Dbi3Q8ABjOro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RL Model Training**"
      ],
      "metadata": {
        "id": "eDieas1XmX-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_save_model(tickers, total_timesteps=300_000, log_dir=\"/content/tb_logs/\"):\n",
        "    env = DummyVecEnv([make_env(tickers)])\n",
        "    model = PPO(\n",
        "        \"MlpPolicy\",\n",
        "        env,\n",
        "        verbose=1,\n",
        "        learning_rate=3e-4,\n",
        "        n_steps=2048,\n",
        "        batch_size=64,\n",
        "        n_epochs=10,\n",
        "        gamma=0.99,\n",
        "        gae_lambda=0.95,\n",
        "        clip_range=0.2,\n",
        "        ent_coef=0.01,\n",
        "        seed=SEED,\n",
        "        tensorboard_log=log_dir,\n",
        "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    )\n",
        "    model.learn(total_timesteps=total_timesteps)\n",
        "    model.save(\"/content/ppo_regime_portfolio_best.zip\")\n",
        "    print(\"Model saved to /content/ppo_regime_portfolio_best.zip\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "zXYiV-UEjYZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Model**..."
      ],
      "metadata": {
        "id": "rvmYH9skm1IF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(path=\"/content/ppo_regime_portfolio_best.zip\"):\n",
        "    model = PPO.load(path)\n",
        "    print(\"Model loaded.\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "a6ss9pTrjc_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Back-Testing and Visualization**"
      ],
      "metadata": {
        "id": "o7XIm9mdm9C9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backtest(model, tickers):\n",
        "    env = EnhancedPortfolioEnv(tickers)\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    portfolio_values = []\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, done, _, _ = env.step(action)\n",
        "        portfolio_values.append(env.total_portfolio_value)\n",
        "    portfolio_values = pd.Series(portfolio_values)\n",
        "    returns = portfolio_values.pct_change().dropna()\n",
        "    sharpe_ratio = returns.mean() / returns.std() * np.sqrt(252)\n",
        "    max_drawdown = ((portfolio_values / portfolio_values.cummax()) - 1).min()\n",
        "    print(f\"Sharpe Ratio: {sharpe_ratio:.4f}\")\n",
        "    print(f\"Max Drawdown: {max_drawdown:.2%}\")\n",
        "    print(f\"Final Portfolio Value: ${portfolio_values.iloc[-1]:,.2f}\")\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(portfolio_values, label='Portfolio Value')\n",
        "    plt.title('Portfolio Value Over Time')\n",
        "    plt.xlabel('Time Step')\n",
        "    plt.ylabel('Portfolio Value ($)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    return portfolio_values"
      ],
      "metadata": {
        "id": "jVcWKaUzjfI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Execution (Training + Learning + Results)** - *may take few hours to train due to 3 lakh timesteps and complexity level*\n",
        "~ *Just be Patient* **(RUN)**"
      ],
      "metadata": {
        "id": "ZaiLJobanGKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tickers = ['SPY', 'TLT', 'GLD', 'QQQ']\n",
        "model = train_and_save_model(tickers, total_timesteps=300_000)\n",
        "backtest(model, tickers)"
      ],
      "metadata": {
        "id": "YRo7KbakjlNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Launch TensorBoard >>>**"
      ],
      "metadata": {
        "id": "rc___WoWnw5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/tb_logs/"
      ],
      "metadata": {
        "id": "kjQQDiXfnvry"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
